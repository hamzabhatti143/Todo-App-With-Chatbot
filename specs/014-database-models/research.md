# Research: Database Models and Migrations

**Feature**: 014-database-models
**Date**: 2026-01-16
**Status**: Completed

## Overview

This document captures the research and technology decisions made for implementing the database layer of the Todo Full-Stack Web Application. The research focuses on SQLModel ORM patterns, Alembic migration strategies, Neon PostgreSQL connection configuration, and best practices for multi-tenant data isolation.

## Technology Decisions

### 1. SQLModel Type Mapping for PostgreSQL

**Decision**: Use SQLModel's built-in type mapping with explicit Field configurations for constraints

**Rationale**:
- SQLModel automatically maps Python types to PostgreSQL types (UUID → uuid, str → varchar, datetime → timestamp)
- `Field()` provides declarative constraints (max_length, default, index, foreign_key) that translate to PostgreSQL DDL
- Type hints enable static analysis and IDE autocomplete while maintaining runtime validation
- Pydantic integration allows reuse of models for API request/response validation

**Implementation Pattern**:
```python
from sqlmodel import SQLModel, Field
from uuid import UUID, uuid4
from datetime import datetime
from typing import Optional

class Task(SQLModel, table=True):
    id: UUID = Field(default_factory=uuid4, primary_key=True)
    title: str = Field(max_length=200, index=True)
    description: Optional[str] = Field(default=None, max_length=1000)
    completed: bool = Field(default=False)
    user_id: UUID = Field(foreign_key="users.id", index=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
```

**Alternatives Considered**:
- **Raw SQLAlchemy Column definitions**: Rejected because it loses Pydantic validation benefits and requires separate schema classes
- **Django ORM**: Rejected because project uses FastAPI, not Django framework
- **Tortoise ORM**: Rejected because SQLModel provides better FastAPI integration and type safety

**References**:
- [SQLModel Documentation - SQL Model Classes](https://sqlmodel.tiangolo.com/)
- [PostgreSQL Data Types](https://www.postgresql.org/docs/current/datatype.html)

---

### 2. Alembic Autogenerate vs Manual Migrations

**Decision**: Use Alembic autogenerate for initial schema creation, manual migrations for complex changes

**Rationale**:
- Autogenerate detects model changes automatically, reducing human error for straightforward schema modifications
- Manual migrations provide fine-grained control for data migrations, complex constraints, and multi-step transformations
- Hybrid approach balances developer velocity (autogenerate) with precision (manual edits)
- Review process ensures autogenerated migrations are correct before applying to production

**Implementation Pattern**:
```bash
# Generate migration from model changes
alembic revision --autogenerate -m "Add conversation and message tables"

# Review generated file in alembic/versions/
# Edit if necessary (add data migrations, adjust constraints)

# Apply migration
alembic upgrade head
```

**Alternatives Considered**:
- **Manual migrations only**: Rejected because it's error-prone and time-consuming for simple schema changes
- **Autogenerate without review**: Rejected because autogenerate can miss complex relationships or produce non-optimal SQL
- **Django migrations**: Rejected because project uses Alembic with SQLAlchemy/SQLModel

**Best Practices**:
1. Always review autogenerated migrations before committing
2. Test migrations on development database first
3. Include both `upgrade()` and `downgrade()` functions
4. Use batch operations for large table alterations
5. Add data migrations when schema changes require data transformation

**References**:
- [Alembic Auto Generating Migrations](https://alembic.sqlalchemy.org/en/latest/autogenerate.html)
- [Alembic Best Practices](https://alembic.sqlalchemy.org/en/latest/cookbook.html)

---

### 3. Connection Pooling for Serverless PostgreSQL (Neon)

**Decision**: Configure SQLAlchemy connection pool with pre-ping enabled and conservative pool sizes

**Rationale**:
- Neon uses serverless architecture that auto-scales connections, so large pool sizes are unnecessary
- `pool_pre_ping=True` validates connections before use, preventing stale connection errors
- Small `pool_size` (5) with `max_overflow` (10) provides sufficient concurrency for typical web application load
- `pool_recycle=3600` (1 hour) prevents long-lived connections from becoming stale
- Serverless databases benefit from connection reuse while avoiding connection exhaustion

**Implementation Pattern**:
```python
from sqlmodel import create_engine

DATABASE_URL = os.getenv("DATABASE_URL")  # postgresql://user:pass@host/db?sslmode=require

engine = create_engine(
    DATABASE_URL,
    echo=True if os.getenv("DEBUG") == "True" else False,
    pool_pre_ping=True,    # Verify connection health before use
    pool_size=5,           # Base connection pool size
    max_overflow=10,       # Additional connections allowed
    pool_recycle=3600,     # Recycle connections after 1 hour
)
```

**Alternatives Considered**:
- **Large pool sizes (50+)**: Rejected because Neon auto-scales and large pools waste resources
- **No connection pooling (NullPool)**: Rejected because creating new connections per request is inefficient
- **pgbouncer external pooler**: Considered for future optimization if traffic exceeds current capacity

**Performance Considerations**:
- Monitor pool usage via SQLAlchemy events to detect pool saturation
- Increase `pool_size` if seeing frequent "pool timeout" errors
- Decrease if database reports too many idle connections

**References**:
- [SQLAlchemy Connection Pooling](https://docs.sqlalchemy.org/en/20/core/pooling.html)
- [Neon Connection Pooling](https://neon.tech/docs/connect/connection-pooling)

---

### 4. Cascade Delete Patterns in SQLModel/SQLAlchemy

**Decision**: Use SQLAlchemy relationship with `cascade="all, delete-orphan"` for parent-child deletion

**Rationale**:
- Conversation-Message relationship requires cascade deletion (delete conversation → delete all messages)
- SQLModel's `Relationship()` accepts SQLAlchemy cascade parameters via `sa_relationship_kwargs`
- Database-level ON DELETE CASCADE provides redundant safety if ORM cascade fails
- `delete-orphan` ensures messages without a conversation are automatically removed

**Implementation Pattern**:
```python
# In Conversation model
from sqlmodel import Relationship
from typing import List

messages: List["Message"] = Relationship(
    back_populates="conversation",
    sa_relationship_kwargs={"cascade": "all, delete-orphan"},
)

# In Message model
conversation: "Conversation" = Relationship(back_populates="messages")

# Database-level constraint (via Alembic migration)
op.create_foreign_key(
    "fk_message_conversation",
    "messages", "conversations",
    ["conversation_id"], ["id"],
    ondelete="CASCADE"
)
```

**Alternatives Considered**:
- **Application-level deletion only**: Rejected because it's not atomic and can leave orphaned records
- **Database-level CASCADE only**: Rejected because ORM wouldn't be aware of deletions, causing cache inconsistencies
- **Soft deletes with deleted_at flag**: Rejected because spec requires hard deletion for data privacy compliance

**Edge Cases Handled**:
- Deleting conversation via ORM session → SQLAlchemy handles cascade
- Deleting conversation via raw SQL → Database CASCADE handles deletion
- Orphaned messages (e.g., manual DELETE in psql) → `delete-orphan` cleans up

**References**:
- [SQLAlchemy Cascades](https://docs.sqlalchemy.org/en/20/orm/cascades.html)
- [PostgreSQL Foreign Key Constraints](https://www.postgresql.org/docs/current/ddl-constraints.html#DDL-CONSTRAINTS-FK)

---

### 5. Index Strategy for Multi-Tenant Data

**Decision**: Create indexes on `user_id` for all user-scoped tables to enable efficient tenant isolation

**Rationale**:
- All queries filter by `user_id` due to multi-tenant architecture (user can only see own data)
- Index on `user_id` converts full table scans into fast index scans
- Compound index not needed for current query patterns (simple equality filters)
- PostgreSQL query planner automatically uses `user_id` index when present in WHERE clause

**Implementation Pattern**:
```python
# In model definition
user_id: UUID = Field(foreign_key="users.id", index=True)

# Generated SQL (via Alembic)
CREATE INDEX ix_tasks_user_id ON tasks(user_id);
CREATE INDEX ix_conversations_user_id ON conversations(user_id);
CREATE INDEX ix_messages_conversation_id ON messages(conversation_id);
```

**Query Performance**:
```sql
-- Without index: Sequential scan (slow for 10K+ rows)
EXPLAIN SELECT * FROM tasks WHERE user_id = 'uuid-here';

-- With index: Index scan (fast even for millions of rows)
Index Scan using ix_tasks_user_id on tasks (cost=0.29..8.31 rows=1 width=...)
```

**Alternatives Considered**:
- **Compound index (user_id, created_at)**: Deferred until query patterns require sorting by created_at frequently
- **Full-text search index on title/content**: Deferred until search feature is implemented
- **Partitioning by user_id**: Rejected because table sizes don't warrant partitioning complexity yet

**Monitoring**:
- Use `EXPLAIN ANALYZE` to verify indexes are utilized
- Monitor slow query logs for sequential scans on indexed columns
- Add indexes incrementally based on production query patterns

**References**:
- [PostgreSQL Indexes](https://www.postgresql.org/docs/current/indexes.html)
- [Multi-Tenant Database Architecture](https://aws.amazon.com/blogs/database/multi-tenant-data-isolation-with-postgresql-row-level-security/)

---

### 6. Timestamp Management with SQLModel Field Defaults

**Decision**: Use `Field(default_factory=datetime.utcnow)` for automatic timestamp generation at creation

**Rationale**:
- `default_factory` calls function at row creation time, ensuring accurate timestamps
- UTC timezone avoids daylight saving time issues and provides consistent global timestamps
- SQLModel automatically includes these fields in INSERT statements with generated values
- No manual timestamp management required in application code

**Implementation Pattern**:
```python
from datetime import datetime
from sqlmodel import Field

class Task(SQLModel, table=True):
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

# Manual update_at management in update endpoints
@router.put("/{task_id}")
async def update_task(task_id: UUID, task_update: TaskUpdate, session: Session = Depends(get_session)):
    task = session.get(Task, task_id)
    for key, value in task_update.model_dump(exclude_unset=True).items():
        setattr(task, key, value)
    task.updated_at = datetime.utcnow()  # Manually update timestamp
    session.add(task)
    session.commit()
    return task
```

**Alternatives Considered**:
- **Database triggers for updated_at**: Rejected because it adds complexity and isn't cross-database compatible
- **SQLAlchemy `onupdate` parameter**: Considered but requires explicit session flush to trigger, less predictable
- **Server default with PostgreSQL NOW()**: Rejected because Python datetime.utcnow() is more testable

**Immutability Design**:
- Message model has only `created_at` (no `updated_at`) because messages are immutable audit records
- Task and Conversation have both because they can be modified by users

**Time Zone Considerations**:
- Store all timestamps in UTC
- Convert to user's local timezone in frontend display layer
- Never store local timestamps in database (causes DST and migration issues)

**References**:
- [SQLModel Fields](https://sqlmodel.tiangelo.com/tutorial/fastapi/simple-hero-api/)
- [Python datetime Best Practices](https://docs.python.org/3/library/datetime.html)

---

## Additional Considerations

### Environment Variable Configuration

All database configuration uses environment variables loaded via `pydantic-settings`:

```python
# app/config.py
from pydantic_settings import BaseSettings

class Settings(BaseSettings):
    database_url: str  # Required
    debug: bool = False

    class Config:
        env_file = ".env"
        case_sensitive = False

settings = Settings()
```

**Required Variables**:
- `DATABASE_URL`: PostgreSQL connection string with SSL mode (e.g., `postgresql://user:pass@neon.tech/db?sslmode=require`)
- `DEBUG`: Enable SQL query echoing for development (default: False)

### Security Best Practices

1. **SQL Injection Prevention**: SQLModel parameterized queries prevent SQL injection
2. **SSL Connections**: Neon requires `?sslmode=require` in connection string
3. **Secrets Management**: Never commit `.env` files; use environment-specific configurations
4. **User Isolation**: All queries MUST filter by `user_id` from JWT token, not URL parameters

### Testing Strategy

1. **Unit Tests**: Test model creation, validation, relationships in isolation (SQLite in-memory)
2. **Integration Tests**: Test migrations, cascade deletes, indexes on PostgreSQL test database
3. **Performance Tests**: Verify query performance with 10K+ records using EXPLAIN ANALYZE

### Migration Workflow

```bash
# 1. Modify models in app/models/
# 2. Generate migration
alembic revision --autogenerate -m "Descriptive message"

# 3. Review and edit migration file
# 4. Test on development database
alembic upgrade head

# 5. Verify schema
alembic current

# 6. Test rollback
alembic downgrade -1
alembic upgrade head
```

---

## Conclusion

The technology stack chosen (SQLModel + Alembic + Neon PostgreSQL) provides a robust, type-safe, and scalable database layer for the application. Key strengths include:

1. **Type Safety**: End-to-end type checking from models to API responses
2. **Developer Experience**: Declarative models with automatic migration generation
3. **Performance**: Indexed queries with connection pooling for serverless architecture
4. **Data Integrity**: Foreign key constraints and cascade deletions enforced at database level
5. **Maintainability**: Version-controlled migrations and environment-based configuration

All decisions align with the project's constitution requirements for database standards and code quality.
